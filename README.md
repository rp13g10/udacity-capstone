# Sparkify - Detecting Customer Churn

## Preface
This project aims to detect customers from the fictional streaming firm Sparkify who are at risk of cancelling their subscription. Two datasets were provided by Udacity to facilitate development of the model:

* S3://udacity-dsnd/sparkify/mini_sparkify_event_data.json
    * A subset of the full dataset, used to speed up script run-times during initial development
* S3://udacity-dsnd/sparkify/sparkify_event_data.json
    * The full dataset (12GB), used to create the final model

The schema for each of the two datasets is the same, so switching between the two is a simple matter. On my own device, running the script from start to finish takes ~7 hours using the full dataset (i9 10900kf processor, 32gb RAM).

An (unlisted) write-up of this project can be found on [medium](https://medium.com/@rp13g10/predicting-customer-churn-with-pyspark-972d70a6993f).

## Requirements
* boto3
* jupyterlab
* numpy
* pandas
* plotly
* pyspark
* scikit-learn

## Notebook Contents

### full_script.ipynb (Primary Script)
* Initialize Script
    * Contains module imports (see requirements above) and a boolean toggle (use_full_dataset) allowing toggling between the full 12GB dataset, and a smaller sample dataset.
    * Configuration options for the local Spark context are also defined here.
* Fetch Data
    * One-off code snippet which downloads the required json files from S3 to the local data directory.
    * Note: This requires some [configuration](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html) in order to authenticate against AWS.
* Explore & Clean Data
    * Generates various summary figures/tables, used to identify useful fields, null values and other anomalies in the data.
    * Most dataframes generated in this section are one-offs, they aren't referenced again in the code. Everything from "Schema exploration" through to the start of "Data Cleaning" can safely be skipped, the generated files are already present in the github repo.
    * Applies some simple logic to fill in missing data (see the "Data Cleaning" section), and parse some text fields into a more useful format (see "Process Text Data").
* Compare Classes
    * Defines the target variable ('userChurnFlag') and generates a variety of plots to compare the characteristics of churned/vs non-churned customers. All figures generated by this section appear in the 'figures' folder, so re-running these cells is strictly optional.
    * Several of the aggregations performed here are re-used as features in the final dataset.
* Feature Engineering
    * Aggregates the source data to one record per userId, creating a wide range of descriptive features for use in the final model.
    * Transform the final dataset into a format which is suitable for use with PySpark classifiers. This includes one-hot-encoding of categorical variables, scaling of numerical variables, and assembly of all features into a single vector per record.
* Modeling
    * Instantiates a pipeline for hyperparameter optimization through cross-validation
    * Fits the model to the data, saving it to 'final_model.bin'
    * Evaluates the performance of the model
    * Perform some further analysis to determine the optimal cutoff threshold for detecting churned customers

### Other Files/Folders
* archive/*
    * Contains some .py files which were used during the initial model creation process. All code in these scripts has now been migrated across to 'full_script.ipynb'
* data/*
    * Used to contain the two source data files once downloaded from S3
    * Contains the raw predictions made for each userId by the final model
* figures/*
    * Contains all figures generated during the EDA process, ROC curves for the final model, and confusion matrices both pre and post threshold tuning.
* images/*
    * Contains all images used in the supporting blog post
* summaries/*
    * Contains all summary tables generated during the EDA process
* final_model.bin
    * The final (fitted) model generated by the main script